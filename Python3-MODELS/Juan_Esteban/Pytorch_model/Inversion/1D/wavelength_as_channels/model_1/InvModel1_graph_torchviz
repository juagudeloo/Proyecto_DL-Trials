digraph {
	graph [size="14.25,14.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140188708513424 [label="
 (80, 80)" fillcolor=darkolivegreen1]
	140188314527936 -> 140188220657680 [dir=none]
	140188220657680 [label="mat1
 (80, 326400)" fillcolor=orange]
	140188314527936 -> 140188220657968 [dir=none]
	140188220657968 [label="mat2
 (326400, 80)" fillcolor=orange]
	140188314527936 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :   (80, 326400)
mat1_sym_strides:    (326400, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :   (326400, 80)
mat2_sym_strides:    (1, 326400)"]
	140188314517856 -> 140188314527936
	140188220656048 [label="simple_conv.10.bias
 (80)" fillcolor=lightblue]
	140188220656048 -> 140188314517856
	140188314517856 [label=AccumulateGrad]
	140188314528464 -> 140188314527936
	140188314528464 -> 140188220658448 [dir=none]
	140188220658448 [label="result1
 (80, 326400)" fillcolor=orange]
	140188314528464 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	140188314525872 -> 140188314528464
	140188314525872 [label="ViewBackward0
------------------------------
self_sym_sizes: (80, 4800, 68)"]
	140188314527744 -> 140188314525872
	140188314527744 -> 140188220658832 [dir=none]
	140188220658832 [label="result
 (80, 4800, 68)" fillcolor=orange]
	140188314527744 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140188314522560 -> 140188314527744
	140188314522560 -> 140188220657488 [dir=none]
	140188220657488 [label="input
 (80, 2400, 67)" fillcolor=orange]
	140188314522560 -> 140188220655760 [dir=none]
	140188220655760 [label="weight
 (4800, 2400, 2)" fillcolor=orange]
	140188314522560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (4800,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140188314522800 -> 140188314522560
	140188314522800 -> 140188220659408 [dir=none]
	140188220659408 [label="result
 (80, 2400, 67)" fillcolor=orange]
	140188314522800 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140188314520496 -> 140188314522800
	140188314520496 -> 140188220656336 [dir=none]
	140188220656336 [label="input
 (80, 1200, 66)" fillcolor=orange]
	140188314520496 -> 140188220654032 [dir=none]
	140188220654032 [label="weight
 (2400, 1200, 2)" fillcolor=orange]
	140188314520496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (2400,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140188314525248 -> 140188314520496
	140188314525248 -> 140188220659984 [dir=none]
	140188220659984 [label="result
 (80, 1200, 66)" fillcolor=orange]
	140188314525248 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140188314522848 -> 140188314525248
	140188314522848 -> 140188220656240 [dir=none]
	140188220656240 [label="input
 (80, 600, 65)" fillcolor=orange]
	140188314522848 -> 140188220654896 [dir=none]
	140188220654896 [label="weight
 (1200, 600, 2)" fillcolor=orange]
	140188314522848 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1200,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140188314524096 -> 140188314522848
	140188314524096 -> 140188220660560 [dir=none]
	140188220660560 [label="result
 (80, 600, 65)" fillcolor=orange]
	140188314524096 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140188314518528 -> 140188314524096
	140188314518528 -> 140188220656144 [dir=none]
	140188220656144 [label="input
 (80, 300, 64)" fillcolor=orange]
	140188314518528 -> 140188220655472 [dir=none]
	140188220655472 [label="weight
 (600, 300, 2)" fillcolor=orange]
	140188314518528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (600,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140188314519728 -> 140188314518528
	140188220655472 [label="simple_conv.0.weight
 (600, 300, 2)" fillcolor=lightblue]
	140188220655472 -> 140188314519728
	140188314519728 [label=AccumulateGrad]
	140188314525968 -> 140188314518528
	140188220655568 [label="simple_conv.0.bias
 (600)" fillcolor=lightblue]
	140188220655568 -> 140188314525968
	140188314525968 [label=AccumulateGrad]
	140188314525632 -> 140188314522848
	140188220654896 [label="simple_conv.2.weight
 (1200, 600, 2)" fillcolor=lightblue]
	140188220654896 -> 140188314525632
	140188314525632 [label=AccumulateGrad]
	140188314523616 -> 140188314522848
	140188220655184 [label="simple_conv.2.bias
 (1200)" fillcolor=lightblue]
	140188220655184 -> 140188314523616
	140188314523616 [label=AccumulateGrad]
	140188314524192 -> 140188314520496
	140188220654032 [label="simple_conv.4.weight
 (2400, 1200, 2)" fillcolor=lightblue]
	140188220654032 -> 140188314524192
	140188314524192 [label=AccumulateGrad]
	140188314525824 -> 140188314520496
	140188220655664 [label="simple_conv.4.bias
 (2400)" fillcolor=lightblue]
	140188220655664 -> 140188314525824
	140188314525824 [label=AccumulateGrad]
	140188314520544 -> 140188314522560
	140188220655760 [label="simple_conv.6.weight
 (4800, 2400, 2)" fillcolor=lightblue]
	140188220655760 -> 140188314520544
	140188314520544 [label=AccumulateGrad]
	140188314522464 -> 140188314522560
	140188220655856 [label="simple_conv.6.bias
 (4800)" fillcolor=lightblue]
	140188220655856 -> 140188314522464
	140188314522464 [label=AccumulateGrad]
	140188314523136 -> 140188314527936
	140188314523136 [label=TBackward0]
	140188314528416 -> 140188314523136
	140188220655952 [label="simple_conv.10.weight
 (80, 326400)" fillcolor=lightblue]
	140188220655952 -> 140188314528416
	140188314528416 [label=AccumulateGrad]
	140188314527936 -> 140188708513424
}
